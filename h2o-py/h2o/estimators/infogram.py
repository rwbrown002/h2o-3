#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# This file is auto-generated by h2o-3/h2o-bindings/bin/gen_python.py
# Copyright 2016 H2O.ai;  Apache License Version 2.0 (see LICENSE for details)
#
from __future__ import absolute_import, division, print_function, unicode_literals

import ast
import json
import h2o
from h2o.utils.typechecks import assert_is_type, is_type, numeric
from h2o.frame import H2OFrame
from h2o.exceptions import H2OValueError
from h2o.estimators.estimator_base import H2OEstimator
from h2o.exceptions import H2OValueError
from h2o.frame import H2OFrame
from h2o.utils.typechecks import assert_is_type, Enum, numeric


class H2OInfoGramEstimator(H2OEstimator):
    """
    Information Diagram

    Given a sensitive/unfair predictors list, InfoGram will add all predictors that contains information on the 
     sensitive/unfair predictors list to the sensitive/unfair predictors list.  It will return a set of predictors that
     do not contain information on the sensitive/unfair list and hence user can build a fair model.  If no sensitive/unfair
     predictor list is given, InfoGram will return a list of core predictors that should be used to build a final model.
     InfoGram can significantly cut down the number of predictors needed to build a model and hence will build a simple
     model that is more interpretable, less susceptible to overfitting, runs faster while providing similar accuracy
     as models built using all attributes.
    """

    algo = "infogram"
    supervised_learning = True

    def __init__(self,
                 model_id=None,  # type: Optional[Union[None, str, H2OEstimator]]
                 training_frame=None,  # type: Optional[Union[None, str, H2OFrame]]
                 validation_frame=None,  # type: Optional[Union[None, str, H2OFrame]]
                 seed=-1,  # type: int
                 keep_cross_validation_models=True,  # type: bool
                 keep_cross_validation_predictions=False,  # type: bool
                 keep_cross_validation_fold_assignment=False,  # type: bool
                 fold_assignment="auto",  # type: Literal["auto", "random", "modulo", "stratified"]
                 fold_column=None,  # type: Optional[str]
                 response_column=None,  # type: Optional[str]
                 ignored_columns=None,  # type: Optional[List[str]]
                 ignore_const_cols=True,  # type: bool
                 score_each_iteration=False,  # type: bool
                 offset_column=None,  # type: Optional[str]
                 weights_column=None,  # type: Optional[str]
                 standardize=False,  # type: bool
                 distribution="auto",  # type: Literal["auto", "bernoulli", "multinomial", "ordinal"]
                 plug_values=None,  # type: Optional[Union[None, str, H2OFrame]]
                 max_iterations=0,  # type: int
                 stopping_rounds=0,  # type: int
                 stopping_metric="auto",  # type: Literal["auto", "deviance", "logloss", "mse", "rmse", "mae", "rmsle", "auc", "aucpr", "lift_top_group", "misclassification", "mean_per_class_error", "custom", "custom_increasing"]
                 stopping_tolerance=0.001,  # type: float
                 balance_classes=False,  # type: bool
                 class_sampling_factors=None,  # type: Optional[List[float]]
                 max_after_balance_size=5.0,  # type: float
                 max_confusion_matrix_size=20,  # type: int
                 max_runtime_secs=0.0,  # type: float
                 custom_metric_func=None,  # type: Optional[str]
                 auc_type="auto",  # type: Literal["auto", "none", "macro_ovr", "weighted_ovr", "macro_ovo", "weighted_ovo"]
                 infogram_algorithm="gbm",  # type: Literal["auto", "deeplearning", "drf", "gbm", "glm", "xgboost"]
                 infogram_algorithm_params=None,  # type: Optional[dict]
                 model_algorithm="gbm",  # type: Literal["auto", "deeplearning", "drf", "gbm", "glm", "xgboost"]
                 model_algorithm_params=None,  # type: Optional[dict]
                 sensitive_attributes=None,  # type: Optional[List[str]]
                 conditional_info_threshold=0.1,  # type: float
                 varimp_threshold=0.1,  # type: float
                 data_fraction=1.0,  # type: float
                 nparallelism=0,  # type: int
                 ntop=50,  # type: int
                 compute_p_values=False,  # type: bool
                 ):
        """
        :param model_id: Destination id for this model; auto-generated if not specified.
               Defaults to ``None``.
        :type model_id: Union[None, str, H2OEstimator], optional
        :param training_frame: Id of the training data frame.
               Defaults to ``None``.
        :type training_frame: Union[None, str, H2OFrame], optional
        :param validation_frame: Id of the validation data frame.
               Defaults to ``None``.
        :type validation_frame: Union[None, str, H2OFrame], optional
        :param seed: Seed for pseudo random number generator (if applicable)
               Defaults to ``-1``.
        :type seed: int
        :param keep_cross_validation_models: Whether to keep the cross-validation models.
               Defaults to ``True``.
        :type keep_cross_validation_models: bool
        :param keep_cross_validation_predictions: Whether to keep the predictions of the cross-validation models.
               Defaults to ``False``.
        :type keep_cross_validation_predictions: bool
        :param keep_cross_validation_fold_assignment: Whether to keep the cross-validation fold assignment.
               Defaults to ``False``.
        :type keep_cross_validation_fold_assignment: bool
        :param fold_assignment: Cross-validation fold assignment scheme, if fold_column is not specified. The
               'Stratified' option will stratify the folds based on the response variable, for classification problems.
               Defaults to ``"auto"``.
        :type fold_assignment: Literal["auto", "random", "modulo", "stratified"]
        :param fold_column: Column with cross-validation fold index assignment per observation.
               Defaults to ``None``.
        :type fold_column: str, optional
        :param response_column: Response variable column.
               Defaults to ``None``.
        :type response_column: str, optional
        :param ignored_columns: Names of columns to ignore for training.
               Defaults to ``None``.
        :type ignored_columns: List[str], optional
        :param ignore_const_cols: Ignore constant columns.
               Defaults to ``True``.
        :type ignore_const_cols: bool
        :param score_each_iteration: Whether to score during each iteration of model training.
               Defaults to ``False``.
        :type score_each_iteration: bool
        :param offset_column: Offset column. This will be added to the combination of columns before applying the link
               function.
               Defaults to ``None``.
        :type offset_column: str, optional
        :param weights_column: Column with observation weights. Giving some observation a weight of zero is equivalent
               to excluding it from the dataset; giving an observation a relative weight of 2 is equivalent to repeating
               that row twice. Negative weights are not allowed. Note: Weights are per-row observation weights and do
               not increase the size of the data frame. This is typically the number of times a row is repeated, but
               non-integer values are supported as well. During training, rows with higher weights matter more, due to
               the larger loss function pre-factor.
               Defaults to ``None``.
        :type weights_column: str, optional
        :param standardize: Standardize numeric columns to have zero mean and unit variance
               Defaults to ``False``.
        :type standardize: bool
        :param distribution: Distribution function
               Defaults to ``"auto"``.
        :type distribution: Literal["auto", "bernoulli", "multinomial", "ordinal"]
        :param plug_values: Plug Values (a single row frame containing values that will be used to impute missing values
               of the training/validation frame, use with conjunction missing_values_handling = PlugValues)
               Defaults to ``None``.
        :type plug_values: Union[None, str, H2OFrame], optional
        :param max_iterations: Maximum number of iterations
               Defaults to ``0``.
        :type max_iterations: int
        :param stopping_rounds: Early stopping based on convergence of stopping_metric. Stop if simple moving average of
               length k of the stopping_metric does not improve for k:=stopping_rounds scoring events (0 to disable)
               Defaults to ``0``.
        :type stopping_rounds: int
        :param stopping_metric: Metric to use for early stopping (AUTO: logloss for classification, deviance for
               regression and anonomaly_score for Isolation Forest). Note that custom and custom_increasing can only be
               used in GBM and DRF with the Python client.
               Defaults to ``"auto"``.
        :type stopping_metric: Literal["auto", "deviance", "logloss", "mse", "rmse", "mae", "rmsle", "auc", "aucpr", "lift_top_group",
               "misclassification", "mean_per_class_error", "custom", "custom_increasing"]
        :param stopping_tolerance: Relative tolerance for metric-based stopping criterion (stop if relative improvement
               is not at least this much)
               Defaults to ``0.001``.
        :type stopping_tolerance: float
        :param balance_classes: Balance training data class counts via over/under-sampling (for imbalanced data).
               Defaults to ``False``.
        :type balance_classes: bool
        :param class_sampling_factors: Desired over/under-sampling ratios per class (in lexicographic order). If not
               specified, sampling factors will be automatically computed to obtain class balance during training.
               Requires balance_classes.
               Defaults to ``None``.
        :type class_sampling_factors: List[float], optional
        :param max_after_balance_size: Maximum relative size of the training data after balancing class counts (can be
               less than 1.0). Requires balance_classes.
               Defaults to ``5.0``.
        :type max_after_balance_size: float
        :param max_confusion_matrix_size: [Deprecated] Maximum size (# classes) for confusion matrices to be printed in
               the Logs
               Defaults to ``20``.
        :type max_confusion_matrix_size: int
        :param max_runtime_secs: Maximum allowed runtime in seconds for model training. Use 0 to disable.
               Defaults to ``0.0``.
        :type max_runtime_secs: float
        :param custom_metric_func: Reference to custom evaluation function, format: `language:keyName=funcName`
               Defaults to ``None``.
        :type custom_metric_func: str, optional
        :param auc_type: Set default multinomial AUC type.
               Defaults to ``"auto"``.
        :type auc_type: Literal["auto", "none", "macro_ovr", "weighted_ovr", "macro_ovo", "weighted_ovo"]
        :param infogram_algorithm: Machine learning algorithm chosen to build the infogram.  AUTO default to GBM
               Defaults to ``"gbm"``.
        :type infogram_algorithm: Literal["auto", "deeplearning", "drf", "gbm", "glm", "xgboost"]
        :param infogram_algorithm_params: parameters specified to the chosen algorithm can be passed to infogram using
               algorithm_params
               Defaults to ``None``.
        :type infogram_algorithm_params: dict, optional
        :param model_algorithm: Machine learning algorithm chosen to build the final model.  AUTO default to GBM
               Defaults to ``"gbm"``.
        :type model_algorithm: Literal["auto", "deeplearning", "drf", "gbm", "glm", "xgboost"]
        :param model_algorithm_params: parameters specified to the chosen final algorithm
               Defaults to ``None``.
        :type model_algorithm_params: dict, optional
        :param sensitive_attributes: predictors that are to be excluded from model due to them being discriminatory or
               inappropriate for whatever reason.
               Defaults to ``None``.
        :type sensitive_attributes: List[str], optional
        :param conditional_info_threshold: conditional information threshold between 0 and 1 that is used to decide
               whether a predictor's conditional information is high enough.  Default to 0.1
               Defaults to ``0.1``.
        :type conditional_info_threshold: float
        :param varimp_threshold: variable importance threshold between 0 and 1 that is used to decide whether a
               predictor's relevance level is high enough.  Default to 0.1
               Defaults to ``0.1``.
        :type varimp_threshold: float
        :param data_fraction: fraction of training frame to use to build the infogram model.  Default to 1.0
               Defaults to ``1.0``.
        :type data_fraction: float
        :param nparallelism: number of models to build in parallel.  Default to 0.0 which is adaptive to the system
               capability
               Defaults to ``0``.
        :type nparallelism: int
        :param ntop: number of top k variables to consider based on the varimp.  Default to 0.0 which is to consider all
               predictors
               Defaults to ``50``.
        :type ntop: int
        :param compute_p_values: If true will calculate the p-value. Default to false
               Defaults to ``False``.
        :type compute_p_values: bool
        """
        super(H2OInfoGramEstimator, self).__init__()
        self._parms = {}
        self._id = self._parms['model_id'] = model_id
        self.training_frame = training_frame
        self.validation_frame = validation_frame
        self.seed = seed
        self.keep_cross_validation_models = keep_cross_validation_models
        self.keep_cross_validation_predictions = keep_cross_validation_predictions
        self.keep_cross_validation_fold_assignment = keep_cross_validation_fold_assignment
        self.fold_assignment = fold_assignment
        self.fold_column = fold_column
        self.response_column = response_column
        self.ignored_columns = ignored_columns
        self.ignore_const_cols = ignore_const_cols
        self.score_each_iteration = score_each_iteration
        self.offset_column = offset_column
        self.weights_column = weights_column
        self.standardize = standardize
        self.distribution = distribution
        self.plug_values = plug_values
        self.max_iterations = max_iterations
        self.stopping_rounds = stopping_rounds
        self.stopping_metric = stopping_metric
        self.stopping_tolerance = stopping_tolerance
        self.balance_classes = balance_classes
        self.class_sampling_factors = class_sampling_factors
        self.max_after_balance_size = max_after_balance_size
        self.max_confusion_matrix_size = max_confusion_matrix_size
        self.max_runtime_secs = max_runtime_secs
        self.custom_metric_func = custom_metric_func
        self.auc_type = auc_type
        self.infogram_algorithm = infogram_algorithm
        self.infogram_algorithm_params = infogram_algorithm_params
        self.model_algorithm = model_algorithm
        self.model_algorithm_params = model_algorithm_params
        self.sensitive_attributes = sensitive_attributes
        self.conditional_info_threshold = conditional_info_threshold
        self.varimp_threshold = varimp_threshold
        self.data_fraction = data_fraction
        self.nparallelism = nparallelism
        self.ntop = ntop
        self.compute_p_values = compute_p_values
        self._parms["_rest_version"] = 3

    @property
    def training_frame(self):
        """
        Id of the training data frame.

        Type: ``Union[None, str, H2OFrame]``.
        """
        return self._parms.get("training_frame")

    @training_frame.setter
    def training_frame(self, training_frame):
        self._parms["training_frame"] = H2OFrame._validate(training_frame, 'training_frame')

    @property
    def validation_frame(self):
        """
        Id of the validation data frame.

        Type: ``Union[None, str, H2OFrame]``.
        """
        return self._parms.get("validation_frame")

    @validation_frame.setter
    def validation_frame(self, validation_frame):
        self._parms["validation_frame"] = H2OFrame._validate(validation_frame, 'validation_frame')

    @property
    def seed(self):
        """
        Seed for pseudo random number generator (if applicable)

        Type: ``int``, defaults to ``-1``.
        """
        return self._parms.get("seed")

    @seed.setter
    def seed(self, seed):
        assert_is_type(seed, None, int)
        self._parms["seed"] = seed

    @property
    def keep_cross_validation_models(self):
        """
        Whether to keep the cross-validation models.

        Type: ``bool``, defaults to ``True``.
        """
        return self._parms.get("keep_cross_validation_models")

    @keep_cross_validation_models.setter
    def keep_cross_validation_models(self, keep_cross_validation_models):
        assert_is_type(keep_cross_validation_models, None, bool)
        self._parms["keep_cross_validation_models"] = keep_cross_validation_models

    @property
    def keep_cross_validation_predictions(self):
        """
        Whether to keep the predictions of the cross-validation models.

        Type: ``bool``, defaults to ``False``.
        """
        return self._parms.get("keep_cross_validation_predictions")

    @keep_cross_validation_predictions.setter
    def keep_cross_validation_predictions(self, keep_cross_validation_predictions):
        assert_is_type(keep_cross_validation_predictions, None, bool)
        self._parms["keep_cross_validation_predictions"] = keep_cross_validation_predictions

    @property
    def keep_cross_validation_fold_assignment(self):
        """
        Whether to keep the cross-validation fold assignment.

        Type: ``bool``, defaults to ``False``.
        """
        return self._parms.get("keep_cross_validation_fold_assignment")

    @keep_cross_validation_fold_assignment.setter
    def keep_cross_validation_fold_assignment(self, keep_cross_validation_fold_assignment):
        assert_is_type(keep_cross_validation_fold_assignment, None, bool)
        self._parms["keep_cross_validation_fold_assignment"] = keep_cross_validation_fold_assignment

    @property
    def fold_assignment(self):
        """
        Cross-validation fold assignment scheme, if fold_column is not specified. The 'Stratified' option will stratify
        the folds based on the response variable, for classification problems.

        Type: ``Literal["auto", "random", "modulo", "stratified"]``, defaults to ``"auto"``.
        """
        return self._parms.get("fold_assignment")

    @fold_assignment.setter
    def fold_assignment(self, fold_assignment):
        assert_is_type(fold_assignment, None, Enum("auto", "random", "modulo", "stratified"))
        self._parms["fold_assignment"] = fold_assignment

    @property
    def fold_column(self):
        """
        Column with cross-validation fold index assignment per observation.

        Type: ``str``.
        """
        return self._parms.get("fold_column")

    @fold_column.setter
    def fold_column(self, fold_column):
        assert_is_type(fold_column, None, str)
        self._parms["fold_column"] = fold_column

    @property
    def response_column(self):
        """
        Response variable column.

        Type: ``str``.
        """
        return self._parms.get("response_column")

    @response_column.setter
    def response_column(self, response_column):
        assert_is_type(response_column, None, str)
        self._parms["response_column"] = response_column

    @property
    def ignored_columns(self):
        """
        Names of columns to ignore for training.

        Type: ``List[str]``.
        """
        return self._parms.get("ignored_columns")

    @ignored_columns.setter
    def ignored_columns(self, ignored_columns):
        assert_is_type(ignored_columns, None, [str])
        self._parms["ignored_columns"] = ignored_columns

    @property
    def ignore_const_cols(self):
        """
        Ignore constant columns.

        Type: ``bool``, defaults to ``True``.
        """
        return self._parms.get("ignore_const_cols")

    @ignore_const_cols.setter
    def ignore_const_cols(self, ignore_const_cols):
        assert_is_type(ignore_const_cols, None, bool)
        self._parms["ignore_const_cols"] = ignore_const_cols

    @property
    def score_each_iteration(self):
        """
        Whether to score during each iteration of model training.

        Type: ``bool``, defaults to ``False``.
        """
        return self._parms.get("score_each_iteration")

    @score_each_iteration.setter
    def score_each_iteration(self, score_each_iteration):
        assert_is_type(score_each_iteration, None, bool)
        self._parms["score_each_iteration"] = score_each_iteration

    @property
    def offset_column(self):
        """
        Offset column. This will be added to the combination of columns before applying the link function.

        Type: ``str``.
        """
        return self._parms.get("offset_column")

    @offset_column.setter
    def offset_column(self, offset_column):
        assert_is_type(offset_column, None, str)
        self._parms["offset_column"] = offset_column

    @property
    def weights_column(self):
        """
        Column with observation weights. Giving some observation a weight of zero is equivalent to excluding it from the
        dataset; giving an observation a relative weight of 2 is equivalent to repeating that row twice. Negative
        weights are not allowed. Note: Weights are per-row observation weights and do not increase the size of the data
        frame. This is typically the number of times a row is repeated, but non-integer values are supported as well.
        During training, rows with higher weights matter more, due to the larger loss function pre-factor.

        Type: ``str``.
        """
        return self._parms.get("weights_column")

    @weights_column.setter
    def weights_column(self, weights_column):
        assert_is_type(weights_column, None, str)
        self._parms["weights_column"] = weights_column

    @property
    def standardize(self):
        """
        Standardize numeric columns to have zero mean and unit variance

        Type: ``bool``, defaults to ``False``.
        """
        return self._parms.get("standardize")

    @standardize.setter
    def standardize(self, standardize):
        assert_is_type(standardize, None, bool)
        self._parms["standardize"] = standardize

    @property
    def distribution(self):
        """
        Distribution function

        Type: ``Literal["auto", "bernoulli", "multinomial", "ordinal"]``, defaults to ``"auto"``.
        """
        return self._parms.get("distribution")

    @distribution.setter
    def distribution(self, distribution):
        assert_is_type(distribution, None, Enum("auto", "bernoulli", "multinomial", "ordinal"))
        self._parms["distribution"] = distribution

    @property
    def plug_values(self):
        """
        Plug Values (a single row frame containing values that will be used to impute missing values of the
        training/validation frame, use with conjunction missing_values_handling = PlugValues)

        Type: ``Union[None, str, H2OFrame]``.
        """
        return self._parms.get("plug_values")

    @plug_values.setter
    def plug_values(self, plug_values):
        self._parms["plug_values"] = H2OFrame._validate(plug_values, 'plug_values')

    @property
    def max_iterations(self):
        """
        Maximum number of iterations

        Type: ``int``, defaults to ``0``.
        """
        return self._parms.get("max_iterations")

    @max_iterations.setter
    def max_iterations(self, max_iterations):
        assert_is_type(max_iterations, None, int)
        self._parms["max_iterations"] = max_iterations

    @property
    def stopping_rounds(self):
        """
        Early stopping based on convergence of stopping_metric. Stop if simple moving average of length k of the
        stopping_metric does not improve for k:=stopping_rounds scoring events (0 to disable)

        Type: ``int``, defaults to ``0``.
        """
        return self._parms.get("stopping_rounds")

    @stopping_rounds.setter
    def stopping_rounds(self, stopping_rounds):
        assert_is_type(stopping_rounds, None, int)
        self._parms["stopping_rounds"] = stopping_rounds

    @property
    def stopping_metric(self):
        """
        Metric to use for early stopping (AUTO: logloss for classification, deviance for regression and anonomaly_score
        for Isolation Forest). Note that custom and custom_increasing can only be used in GBM and DRF with the Python
        client.

        Type: ``Literal["auto", "deviance", "logloss", "mse", "rmse", "mae", "rmsle", "auc", "aucpr", "lift_top_group",
        "misclassification", "mean_per_class_error", "custom", "custom_increasing"]``, defaults to ``"auto"``.
        """
        return self._parms.get("stopping_metric")

    @stopping_metric.setter
    def stopping_metric(self, stopping_metric):
        assert_is_type(stopping_metric, None, Enum("auto", "deviance", "logloss", "mse", "rmse", "mae", "rmsle", "auc", "aucpr", "lift_top_group", "misclassification", "mean_per_class_error", "custom", "custom_increasing"))
        self._parms["stopping_metric"] = stopping_metric

    @property
    def stopping_tolerance(self):
        """
        Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this much)

        Type: ``float``, defaults to ``0.001``.
        """
        return self._parms.get("stopping_tolerance")

    @stopping_tolerance.setter
    def stopping_tolerance(self, stopping_tolerance):
        assert_is_type(stopping_tolerance, None, numeric)
        self._parms["stopping_tolerance"] = stopping_tolerance

    @property
    def balance_classes(self):
        """
        Balance training data class counts via over/under-sampling (for imbalanced data).

        Type: ``bool``, defaults to ``False``.
        """
        return self._parms.get("balance_classes")

    @balance_classes.setter
    def balance_classes(self, balance_classes):
        assert_is_type(balance_classes, None, bool)
        self._parms["balance_classes"] = balance_classes

    @property
    def class_sampling_factors(self):
        """
        Desired over/under-sampling ratios per class (in lexicographic order). If not specified, sampling factors will
        be automatically computed to obtain class balance during training. Requires balance_classes.

        Type: ``List[float]``.
        """
        return self._parms.get("class_sampling_factors")

    @class_sampling_factors.setter
    def class_sampling_factors(self, class_sampling_factors):
        assert_is_type(class_sampling_factors, None, [float])
        self._parms["class_sampling_factors"] = class_sampling_factors

    @property
    def max_after_balance_size(self):
        """
        Maximum relative size of the training data after balancing class counts (can be less than 1.0). Requires
        balance_classes.

        Type: ``float``, defaults to ``5.0``.
        """
        return self._parms.get("max_after_balance_size")

    @max_after_balance_size.setter
    def max_after_balance_size(self, max_after_balance_size):
        assert_is_type(max_after_balance_size, None, float)
        self._parms["max_after_balance_size"] = max_after_balance_size

    @property
    def max_confusion_matrix_size(self):
        """
        [Deprecated] Maximum size (# classes) for confusion matrices to be printed in the Logs

        Type: ``int``, defaults to ``20``.
        """
        return self._parms.get("max_confusion_matrix_size")

    @max_confusion_matrix_size.setter
    def max_confusion_matrix_size(self, max_confusion_matrix_size):
        assert_is_type(max_confusion_matrix_size, None, int)
        self._parms["max_confusion_matrix_size"] = max_confusion_matrix_size

    @property
    def max_runtime_secs(self):
        """
        Maximum allowed runtime in seconds for model training. Use 0 to disable.

        Type: ``float``, defaults to ``0.0``.
        """
        return self._parms.get("max_runtime_secs")

    @max_runtime_secs.setter
    def max_runtime_secs(self, max_runtime_secs):
        assert_is_type(max_runtime_secs, None, numeric)
        self._parms["max_runtime_secs"] = max_runtime_secs

    @property
    def custom_metric_func(self):
        """
        Reference to custom evaluation function, format: `language:keyName=funcName`

        Type: ``str``.
        """
        return self._parms.get("custom_metric_func")

    @custom_metric_func.setter
    def custom_metric_func(self, custom_metric_func):
        assert_is_type(custom_metric_func, None, str)
        self._parms["custom_metric_func"] = custom_metric_func

    @property
    def auc_type(self):
        """
        Set default multinomial AUC type.

        Type: ``Literal["auto", "none", "macro_ovr", "weighted_ovr", "macro_ovo", "weighted_ovo"]``, defaults to
        ``"auto"``.
        """
        return self._parms.get("auc_type")

    @auc_type.setter
    def auc_type(self, auc_type):
        assert_is_type(auc_type, None, Enum("auto", "none", "macro_ovr", "weighted_ovr", "macro_ovo", "weighted_ovo"))
        self._parms["auc_type"] = auc_type

    @property
    def infogram_algorithm(self):
        """
        Machine learning algorithm chosen to build the infogram.  AUTO default to GBM

        Type: ``Literal["auto", "deeplearning", "drf", "gbm", "glm", "xgboost"]``, defaults to ``"gbm"``.
        """
        return self._parms.get("infogram_algorithm")

    @infogram_algorithm.setter
    def infogram_algorithm(self, infogram_algorithm):
        assert_is_type(infogram_algorithm, None, Enum("auto", "deeplearning", "drf", "gbm", "glm", "xgboost"))
        self._parms["infogram_algorithm"] = infogram_algorithm

    @property
    def infogram_algorithm_params(self):
        """
        parameters specified to the chosen algorithm can be passed to infogram using algorithm_params

        Type: ``dict``.
        """
        if self._parms.get("infogram_algorithm_params") != None:
            infogram_algorithm_params_dict =  ast.literal_eval(self._parms.get("infogram_algorithm_params"))
            for k in infogram_algorithm_params_dict:
                if len(infogram_algorithm_params_dict[k]) == 1: #single parameter
                    infogram_algorithm_params_dict[k] = infogram_algorithm_params_dict[k][0]
            return infogram_algorithm_params_dict
        else:
            return self._parms.get("infogram_algorithm_params")

    @infogram_algorithm_params.setter
    def infogram_algorithm_params(self, infogram_algorithm_params):
        assert_is_type(infogram_algorithm_params, None, dict)
        if infogram_algorithm_params is not None and infogram_algorithm_params != "":
            for k in infogram_algorithm_params:
                if ("[" and "]") not in str(infogram_algorithm_params[k]):
                    infogram_algorithm_params[k] = [infogram_algorithm_params[k]]
            self._parms["infogram_algorithm_params"] = str(json.dumps(infogram_algorithm_params))
        else:
            self._parms["infogram_algorithm_params"] = None

    @property
    def model_algorithm(self):
        """
        Machine learning algorithm chosen to build the final model.  AUTO default to GBM

        Type: ``Literal["auto", "deeplearning", "drf", "gbm", "glm", "xgboost"]``, defaults to ``"gbm"``.
        """
        return self._parms.get("model_algorithm")

    @model_algorithm.setter
    def model_algorithm(self, model_algorithm):
        assert_is_type(model_algorithm, None, Enum("auto", "deeplearning", "drf", "gbm", "glm", "xgboost"))
        self._parms["model_algorithm"] = model_algorithm

    @property
    def model_algorithm_params(self):
        """
        parameters specified to the chosen final algorithm

        Type: ``dict``.
        """
        if self._parms.get("model_algorithm_params") != None:
            model_algorithm_params_dict =  ast.literal_eval(self._parms.get("model_algorithm_params"))
            for k in model_algorithm_params_dict:
                if len(model_algorithm_params_dict[k]) == 1: #single parameter
                    model_algorithm_params_dict[k] = model_algorithm_params_dict[k][0]
            return model_algorithm_params_dict
        else:
            return self._parms.get("model_algorithm_params")

    @model_algorithm_params.setter
    def model_algorithm_params(self, model_algorithm_params):
        assert_is_type(model_algorithm_params, None, dict)
        if model_algorithm_params is not None and model_algorithm_params != "":
            for k in model_algorithm_params:
                if ("[" and "]") not in str(model_algorithm_params[k]):
                    model_algorithm_params[k] = [model_algorithm_params[k]]
            self._parms["model_algorithm_params"] = str(json.dumps(model_algorithm_params))
        else:
            self._parms["model_algorithm_params"] = None

    @property
    def sensitive_attributes(self):
        """
        predictors that are to be excluded from model due to them being discriminatory or inappropriate for whatever
        reason.

        Type: ``List[str]``.
        """
        return self._parms.get("sensitive_attributes")

    @sensitive_attributes.setter
    def sensitive_attributes(self, sensitive_attributes):
        assert_is_type(sensitive_attributes, None, [str])
        self._parms["sensitive_attributes"] = sensitive_attributes

    @property
    def conditional_info_threshold(self):
        """
        conditional information threshold between 0 and 1 that is used to decide whether a predictor's conditional
        information is high enough.  Default to 0.1

        Type: ``float``, defaults to ``0.1``.
        """
        return self._parms.get("conditional_info_threshold")

    @conditional_info_threshold.setter
    def conditional_info_threshold(self, conditional_info_threshold):
        assert_is_type(conditional_info_threshold, None, numeric)
        self._parms["conditional_info_threshold"] = conditional_info_threshold

    @property
    def varimp_threshold(self):
        """
        variable importance threshold between 0 and 1 that is used to decide whether a predictor's relevance level is
        high enough.  Default to 0.1

        Type: ``float``, defaults to ``0.1``.
        """
        return self._parms.get("varimp_threshold")

    @varimp_threshold.setter
    def varimp_threshold(self, varimp_threshold):
        assert_is_type(varimp_threshold, None, numeric)
        self._parms["varimp_threshold"] = varimp_threshold

    @property
    def data_fraction(self):
        """
        fraction of training frame to use to build the infogram model.  Default to 1.0

        Type: ``float``, defaults to ``1.0``.
        """
        return self._parms.get("data_fraction")

    @data_fraction.setter
    def data_fraction(self, data_fraction):
        assert_is_type(data_fraction, None, numeric)
        self._parms["data_fraction"] = data_fraction

    @property
    def nparallelism(self):
        """
        number of models to build in parallel.  Default to 0.0 which is adaptive to the system capability

        Type: ``int``, defaults to ``0``.
        """
        return self._parms.get("nparallelism")

    @nparallelism.setter
    def nparallelism(self, nparallelism):
        assert_is_type(nparallelism, None, int)
        self._parms["nparallelism"] = nparallelism

    @property
    def ntop(self):
        """
        number of top k variables to consider based on the varimp.  Default to 0.0 which is to consider all predictors

        Type: ``int``, defaults to ``50``.
        """
        return self._parms.get("ntop")

    @ntop.setter
    def ntop(self, ntop):
        assert_is_type(ntop, None, int)
        self._parms["ntop"] = ntop

    @property
    def compute_p_values(self):
        """
        If true will calculate the p-value. Default to false

        Type: ``bool``, defaults to ``False``.
        """
        return self._parms.get("compute_p_values")

    @compute_p_values.setter
    def compute_p_values(self, compute_p_values):
        assert_is_type(compute_p_values, None, bool)
        self._parms["compute_p_values"] = compute_p_values


    def get_relevance_cmi_frame(self):
        """
        Get the relevance and CMI for all attributes returned by InfoGram as an H2O Frame.
        :param self: 
        :return: H2OFrame
        """
        keyString = self._model_json["output"]["relevance_cmi_key"]
        if not (keyString == None):
            return h2o.get_frame(keyString)
        else:
            return None

    def get_admissible_attributes(self):
        """
        Get the admissible attributes
        :param self: 
        :return: 
        """
        if not (self._model_json["output"]["admissible_features"] == None):
            return self._model_json["output"]["admissible_features"]
        else:
            return None

    def get_admissible_relevance(self):
        """
        Get the relevance of admissible attributes
        :param self: 
        :return: 
        """
        if not (self._model_json["output"]["admissible_relevance"] == None):
            return self._model_json["output"]["admissible_relevance"]
        else:
            return None

    def get_admissible_cmi(self):
        """
        Get the normalized cmi of admissible attributes
        :param self: 
        :return: 
        """
        if not (self._model_json["output"]["admissible_cmi"] == None):
            return self._model_json["output"]["admissible_cmi"]
        else:
            return None

    def get_admissible_cmi_raw(self):
        """
        Get the raw cmi of admissible attributes
        :param self: 
        :return: 
        """
        if not (self._model_json["output"]["admissible_cmi_raw"] == None):
            return self._model_json["output"]["admissible_cmi_raw"]
        else:
            return None

    def get_all_predictor_relevance(self):
        """
        Get relevance of all predictors
        :param self: 
        :return: two tuples, first one is predictor names and second one is relevance
        """
        if not (self._model_json["output"]["all_predictor_names"] == None):
            return self._model_json["output"]["all_predictor_names"], self._model_json["output"]["relevance"]
        else:
            return None


    def get_all_predictor_cmi(self):
        """
        Get normalized cmi of all predictors.
        :param self: 
        :return: two tuples, first one is predictor names and second one is cmi
        """
        if not (self._model_json["output"]["all_predictor_names"] == None):
            return self._model_json["output"]["all_predictor_names"], self._model_json["output"]["cmi"]
        else:
            return None


    def get_all_predictor_cmi_raw(self):
        """
        Get raw cmi of all predictors.
        :param self: 
        :return: two tuples, first one is predictor names and second one is cmi
        """
        if not (self._model_json["output"]["all_predictor_names"] == None):
            return self._model_json["output"]["all_predictor_names"], self._model_json["output"]["cmi_raw"]
        else:
            return None

    # Override train method to support infogram needs
    def train(self, x=None, y=None, training_frame=None, blending_frame=None, verbose=False, **kwargs):
        sup = super(self.__class__, self)

        def extend_parms(parms): # add parameter checks specific to infogram
            training_col_names = training_frame.names
            if not(parms["conditional_info_threshold"] == None):
                assert_is_type(parms["conditional_info_threshold"], numeric)
                assert parms["conditional_info_threshold"] >= 0 and parms["conditional_info_threshold"] <= 1,\
                    "conditional_info_threshold should be between 0 and 1."
            if not(parms["varimp_threshold"] == None):
                assert_is_type(parms["varimp_threshold"], numeric)
                assert parms["varimp_threshold"] >= 0 and parms["varimp_threshold"] <= 1, "varimp_threshold should be" \
                                                                                      " between 0 and 1."
            if not(parms["data_fraction"] == None):
                assert_is_type(parms["data_fraction"], numeric)
                assert parms["data_fraction"] > 0 and parms["data_fraction"] <= 1, "data_fraction should exceed 0" \
                                                                               " and <= 1."

        parms = sup._make_parms(x,y,training_frame, extend_parms_fn = extend_parms, **kwargs)
        sup._train(parms, verbose=verbose)
        # can probably get rid of model attributes that Erin does not want here
        return self
